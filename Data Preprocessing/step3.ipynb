{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This worksheet is identifying the out of dictionary words."
      ],
      "metadata": {
        "id": "kBrmt2K_LqDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHPCam6-Hdso",
        "outputId": "8b38a00b-6e64-4891-b90e-c342cc755872"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9DKnjh8xHLWg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import words, wordnet\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('words')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5AvXud2HUZA",
        "outputId": "e67b6899-c9cc-4196-c555-fd1b375661ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk_words = set(words.words())"
      ],
      "metadata": {
        "id": "S-uV7jmjHYho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "WuvF_fllHYkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_oov_words(sentence):\n",
        "    \"\"\"Identifies words not found in the dictionary or spaCy model.\"\"\"\n",
        "    if not isinstance(sentence, str):  # Handle missing values\n",
        "        return \"\"\n",
        "\n",
        "    tokens = [word.lower() for word in nltk.word_tokenize(sentence) if word.isalpha()]\n",
        "    oov_words = [\n",
        "        word for word in tokens if word not in nltk_words and not wordnet.synsets(word) and word not in nlp.vocab\n",
        "    ]\n",
        "\n",
        "    return \", \".join(oov_words) if oov_words else \"None\""
      ],
      "metadata": {
        "id": "yPSiLkLAHYmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Dataset/cleaned_train_post_step2.csv\")"
      ],
      "metadata": {
        "id": "Sl8qLOo8H87h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"out_of_vocabulary_words\"] = df[\"english_sen\"].apply(get_oov_words)"
      ],
      "metadata": {
        "id": "F6tq-Q2fH9A4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"/content/drive/MyDrive/Dataset/cleaned_train_post_step3.csv\", index=False)"
      ],
      "metadata": {
        "id": "4RoYTA57H9DZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Processing complete! Check 'updated_dataset.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32qGB4VkH9Gg",
        "outputId": "a9fcb0b9-f0a9-4f6e-ad11-bbe5b4b0b003"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing complete! Check 'updated_dataset.csv'\n"
          ]
        }
      ]
    }
  ]
}